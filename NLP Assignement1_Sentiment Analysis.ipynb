{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71903986",
   "metadata": {},
   "source": [
    "\n",
    "INSTRUCTIONS: Just Upload the file to Jupyter Notebook or Google Colab and run each cell one after the other                                      \n",
    "NOTE: For Jupyter notebook, you will have to make sure you have the relevant packages and libraries imported below downloaded into you Anaconda environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d69a5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6c46a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directiory:  C:\\Users\\ephra\\Documents\\UCT Jupyter Notebook\\NLP/afrisent-semeval-2023\n",
      "C:\\Users\\ephra\\Documents\\UCT Jupyter Notebook\\NLP\\afrisent-semeval-2023\n",
      "Already up to date.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From https://github.com/afrisenti-semeval/afrisent-semeval-2023\n",
      " * branch            HEAD       -> FETCH_HEAD\n"
     ]
    }
   ],
   "source": [
    "# Download dataset\n",
    "PROJECT_DIR = os.getcwd() + '/afrisent-semeval-2023'\n",
    "print('Current directiory: ', PROJECT_DIR)\n",
    "PROJECT_GITHUB_URL = 'https://github.com/afrisenti-semeval/afrisent-semeval-2023.git'\n",
    "\n",
    "if not os.path.isdir(PROJECT_DIR):\n",
    "  !git clone {PROJECT_GITHUB_URL}\n",
    "else:\n",
    "  %cd {PROJECT_DIR}\n",
    "  !git pull {PROJECT_GITHUB_URL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b59dbe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'twi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4856a722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory:  C:\\Users\\ephra\\Documents\\UCT Jupyter Notebook\\NLP/afrisent-semeval-2023/data/twi\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "DATA_DIR = f'{PROJECT_DIR}/data/{language}'\n",
    "print('Data directory: ', DATA_DIR)\n",
    "\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.tsv', sep='\\t', names=['text', 'label'], header=0)\n",
    "dev_df = pd.read_csv(f'{DATA_DIR}/dev.tsv', sep='\\t', names=['text', 'label'], header=0)\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.tsv', sep='\\t', names=['text', 'label'], header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db8696f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  (3481, 2)\n",
      "Dev shape:  (388, 2)\n",
      "Test shape:  (949, 2)\n"
     ]
    }
   ],
   "source": [
    "# Data Shape\n",
    "print('Train shape: ', train_df.shape)\n",
    "print('Dev shape: ', dev_df.shape)\n",
    "print('Test shape: ', test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8400be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2235</th>\n",
       "      <td>3herh oh wo yaw sarpong ankasa ni s3n wo haw n...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>wei na wo di p3 sporting abo no ðŸ¤£</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1666</th>\n",
       "      <td>woyem ahye wo wnnnn</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>ring koraa yedi na header madi wo awie ðŸ˜‚</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3458</th>\n",
       "      <td>ne nyinaa biy3b3di sallah no paa</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>me yem koraa hyehye me ðŸ˜­ðŸ˜­ðŸ˜­</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2147</th>\n",
       "      <td>wode33 fa ps5 sika no twa me surprise</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>sister debbie is pretty ðŸ¤© eii meyem kraa ahye me</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2417</th>\n",
       "      <td>woa wasa nsuo kakra aka no wo san de awarÉ› asÉ›...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>ei asi enti min ka ho bi</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text     label\n",
       "2235  3herh oh wo yaw sarpong ankasa ni s3n wo haw n...  positive\n",
       "1025                  wei na wo di p3 sporting abo no ðŸ¤£  negative\n",
       "1666                                woyem ahye wo wnnnn   neutral\n",
       "416            ring koraa yedi na header madi wo awie ðŸ˜‚  negative\n",
       "3458                   ne nyinaa biy3b3di sallah no paa  positive\n",
       "899                          me yem koraa hyehye me ðŸ˜­ðŸ˜­ðŸ˜­  negative\n",
       "2147              wode33 fa ps5 sika no twa me surprise  positive\n",
       "3197   sister debbie is pretty ðŸ¤© eii meyem kraa ahye me  positive\n",
       "2417  woa wasa nsuo kakra aka no wo san de awarÉ› asÉ›...  positive\n",
       "402                            ei asi enti min ka ho bi  negative"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display data\n",
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d564503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discard neutral examples\n",
    "train_df = train_df[train_df['label'] != 'neutral']\n",
    "dev_df = dev_df[dev_df['label'] != 'neutral']\n",
    "test_df = test_df[test_df['label'] != 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2c61a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2390</th>\n",
       "      <td>aseey aba time na album nu b3 drop na ma tor d...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>that guy nyansa baako sei wo nni bi</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>wo ti awu forken wo</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3134</th>\n",
       "      <td>yÉ› digi online gang keep the secrets in mind g...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291</th>\n",
       "      <td>nyame b3tua klopp ka papaaapa</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>nyansa doduor yÉ› gyimie ðŸ¤£ðŸ¤£ðŸ¤£</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>ahhhh kumasi foÉ” ne jon deÉ›ðŸ˜¹ðŸ˜¹ðŸ˜¹</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3263</th>\n",
       "      <td>ma me y3 wa danfo err</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>aaaah na wo brofo left left bÉ›n nni</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2887</th>\n",
       "      <td>agye wonmo nsamu ntiaa but dont worry cause 3n...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3362</th>\n",
       "      <td>ma hwehwÉ› mu ayi deÉ› É›yÉ› oo akÉ”nÉ”</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>everyday u dey tweet ashawo season ashawo seas...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>maban denden ne wo jesus christ awurade mahuod...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2329</th>\n",
       "      <td>eeei this de3 nyame ay3 bi ampa</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>eeii hwan ba nieðŸ¤”ðŸ¤” É›na É”ka nsÉ›m hunu sei</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>woy3 kwasia paaaaðŸ˜‚ðŸ˜‚ðŸ˜‚anka wob3n me aa anka wob3...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2464</th>\n",
       "      <td>sa kÉ” deÉ› huuh hi</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>eiiiiii na y3 nna y3 b3 br3 wa ha sendi ma lec...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3112</th>\n",
       "      <td>greetings ðŸ–– oooo mo ne adwuma pa</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>kumerica y3 wo ade paaaa</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text     label\n",
       "2390  aseey aba time na album nu b3 drop na ma tor d...  positive\n",
       "659                 that guy nyansa baako sei wo nni bi  negative\n",
       "532                                 wo ti awu forken wo  negative\n",
       "3134  yÉ› digi online gang keep the secrets in mind g...  positive\n",
       "1291                      nyame b3tua klopp ka papaaapa  negative\n",
       "1186                        nyansa doduor yÉ› gyimie ðŸ¤£ðŸ¤£ðŸ¤£  negative\n",
       "830                      ahhhh kumasi foÉ” ne jon deÉ›ðŸ˜¹ðŸ˜¹ðŸ˜¹  negative\n",
       "3263                              ma me y3 wa danfo err  positive\n",
       "974                 aaaah na wo brofo left left bÉ›n nni  negative\n",
       "2887  agye wonmo nsamu ntiaa but dont worry cause 3n...  positive\n",
       "3362                  ma hwehwÉ› mu ayi deÉ› É›yÉ› oo akÉ”nÉ”  positive\n",
       "256   everyday u dey tweet ashawo season ashawo seas...  negative\n",
       "2009  maban denden ne wo jesus christ awurade mahuod...  positive\n",
       "2329                    eeei this de3 nyame ay3 bi ampa  positive\n",
       "1063           eeii hwan ba nieðŸ¤”ðŸ¤” É›na É”ka nsÉ›m hunu sei  negative\n",
       "562   woy3 kwasia paaaaðŸ˜‚ðŸ˜‚ðŸ˜‚anka wob3n me aa anka wob3...  negative\n",
       "2464                                  sa kÉ” deÉ› huuh hi  positive\n",
       "1229  eiiiiii na y3 nna y3 b3 br3 wa ha sendi ma lec...  negative\n",
       "3112                   greetings ðŸ–– oooo mo ne adwuma pa  positive\n",
       "2028                           kumerica y3 wo ade paaaa  positive"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a0c9808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    # Replace URLS with [URL]\n",
    "    text = re.sub(r'http\\S+', '[URL]', text)\n",
    "    \n",
    "    # Replace numbers with [NUM]\n",
    "    text = re.sub(r'\\d+', '[NUM]', text)\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove emojis and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n",
    "    \n",
    "    # Ensure there are no multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove trailing spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fede99cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text in the dataframes\n",
    "train_df['text'] = train_df['text'].apply(clean)\n",
    "dev_df['text'] = dev_df['text'].apply(clean)\n",
    "test_df['text'] = test_df['text'].apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ed4cc1",
   "metadata": {},
   "source": [
    "BITE PAIR ENCODING TOKENIZATION CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2b06995",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer():\n",
    "    def __init__(self, sentences, vocab_size):\n",
    "        self.sentences = sentences\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_freqs = defaultdict(int)\n",
    "        self.splits = {}\n",
    "        self.merges = {}\n",
    "\n",
    "        # Initialize word frequencies\n",
    "        for sentence in self.sentences:\n",
    "            for word in sentence:\n",
    "                self.word_freqs[word] += 1\n",
    "\n",
    "    def train(self):\n",
    "        for word in self.word_freqs:\n",
    "            self.splits[word] = [char for char in word]\n",
    "        \n",
    "        while len(self.splits) < self.vocab_size:\n",
    "            pair_freqs = self.compute_pair_freqs()\n",
    "            if not pair_freqs:\n",
    "                break\n",
    "            best_pair = max(pair_freqs, key=pair_freqs.get)\n",
    "            self.merge_pair(best_pair[0], best_pair[1])\n",
    "        \n",
    "        return self.merges\n",
    "\n",
    "    def compute_pair_freqs(self):\n",
    "        pairs = defaultdict(int)\n",
    "        for word, freq in self.word_freqs.items():\n",
    "            symbols = self.splits[word]\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "        return pairs\n",
    "\n",
    "    def merge_pair(self, a, b):\n",
    "        merge = a + b\n",
    "        for word in list(self.splits):\n",
    "            split = self.splits[word]\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == a and split[i + 1] == b:\n",
    "                    split = split[:i] + [merge] + split[i + 2:]\n",
    "                else:\n",
    "                    i += 1\n",
    "            self.splits[word] = split\n",
    "        self.merges[(a, b)] = merge\n",
    "        return self.splits\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        pre_tokenized_text = text.split()\n",
    "        splits_text = [[char for char in word] for word in pre_tokenized_text]\n",
    "\n",
    "        for pair, merge in self.merges.items():\n",
    "            for idx, split in enumerate(splits_text):\n",
    "                i = 0\n",
    "                while i < len(split) - 1:\n",
    "                    if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                        split = split[:i] + [merge] + split[i + 2:]\n",
    "                    else:\n",
    "                        i += 1\n",
    "                splits_text[idx] = split\n",
    "        result = sum(splits_text, [])\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9763b669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "train_corpus = [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence']]\n",
    "bpe_tokenizer = BPETokenizer(train_corpus, vocab_size=50)\n",
    "bpe_tokenizer.train()\n",
    "print(bpe_tokenizer.tokenize(\"This is a sentence\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9c09f4",
   "metadata": {},
   "source": [
    "TRAINING BITE PAIR TOKENIZATION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a2e66e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train BPE\n",
    "train_corpus = train_df['text'].tolist()\n",
    "tokenized_train_corpus = [list(word) for sentence in train_corpus for word in sentence.split()]\n",
    "\n",
    "bpe = BPETokenizer(tokenized_train_corpus, vocab_size=1000)\n",
    "merges = bpe.train()\n",
    "\n",
    "# Apply BPE tokenization to our dataset\n",
    "train_df['bpe_text'] = train_df['text'].apply(lambda x: ' '.join(bpe.tokenize(x)))\n",
    "dev_df['bpe_text'] = dev_df['text'].apply(lambda x: ' '.join(bpe.tokenize(x)))\n",
    "test_df['bpe_text'] = test_df['text'].apply(lambda x: ' '.join(bpe.tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "582ab293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save merges to a file after training\n",
    "with open('bpe_merges.pkl', 'wb') as f:\n",
    "    pickle.dump(merges, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26611de3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>bpe_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2118</th>\n",
       "      <td>masamasa bolebamboi ooo kumasi ooo theyve take...</td>\n",
       "      <td>positive</td>\n",
       "      <td>m a s a m a s a b o l e b a m b o i o o o k u ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2366</th>\n",
       "      <td>reality check be what lol ynumti sika so numnso</td>\n",
       "      <td>positive</td>\n",
       "      <td>r e a l i t y c h e c k b e w h a t l o l y n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3476</th>\n",
       "      <td>kasa gu ma sum na me gyedi</td>\n",
       "      <td>positive</td>\n",
       "      <td>k a s a g u m a s u m n a m e g y e d i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>wei wei y de gye cup</td>\n",
       "      <td>negative</td>\n",
       "      <td>w e i w e i y d e g y e c u p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2782</th>\n",
       "      <td>numka wo nantini a na aka wo to</td>\n",
       "      <td>positive</td>\n",
       "      <td>n u m k a w o n a n t i n i a n a a k a w o t o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537</th>\n",
       "      <td>ynum ynum dom wo kum apem a apem bnumba king p...</td>\n",
       "      <td>positive</td>\n",
       "      <td>y n u m y n u m d o m w o k u m a p e m a a p ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2172</th>\n",
       "      <td>sika no asa sesiaa</td>\n",
       "      <td>positive</td>\n",
       "      <td>s i k a n o a s a s e s i a a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>wo mpo wo te kurom mpo na wagyimi saa no numne...</td>\n",
       "      <td>negative</td>\n",
       "      <td>w o m p o w o t e k u r o m m p o n a w a g y ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>theyre all de same bro nkwasiafo nkoaaaa</td>\n",
       "      <td>negative</td>\n",
       "      <td>t h e y r e a l l d e s a m e b r o n k w a s ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>buy wig na ti ti fa tari h</td>\n",
       "      <td>negative</td>\n",
       "      <td>b u y w i g n a t i t i f a t a r i h</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text     label  \\\n",
       "2118  masamasa bolebamboi ooo kumasi ooo theyve take...  positive   \n",
       "2366    reality check be what lol ynumti sika so numnso  positive   \n",
       "3476                         kasa gu ma sum na me gyedi  positive   \n",
       "1051                               wei wei y de gye cup  negative   \n",
       "2782                    numka wo nantini a na aka wo to  positive   \n",
       "2537  ynum ynum dom wo kum apem a apem bnumba king p...  positive   \n",
       "2172                                 sika no asa sesiaa  positive   \n",
       "779   wo mpo wo te kurom mpo na wagyimi saa no numne...  negative   \n",
       "910            theyre all de same bro nkwasiafo nkoaaaa  negative   \n",
       "194                          buy wig na ti ti fa tari h  negative   \n",
       "\n",
       "                                               bpe_text  \n",
       "2118  m a s a m a s a b o l e b a m b o i o o o k u ...  \n",
       "2366  r e a l i t y c h e c k b e w h a t l o l y n ...  \n",
       "3476            k a s a g u m a s u m n a m e g y e d i  \n",
       "1051                      w e i w e i y d e g y e c u p  \n",
       "2782    n u m k a w o n a n t i n i a n a a k a w o t o  \n",
       "2537  y n u m y n u m d o m w o k u m a p e m a a p ...  \n",
       "2172                      s i k a n o a s a s e s i a a  \n",
       "779   w o m p o w o t e k u r o m m p o n a w a g y ...  \n",
       "910   t h e y r e a l l d e s a m e b r o n k w a s ...  \n",
       "194               b u y w i g n a t i t i f a t a r i h  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7006896a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty documents in train set: 0\n"
     ]
    }
   ],
   "source": [
    "# Check if there are empty documents\n",
    "empty_docs = train_df[train_df['bpe_text'].str.strip().eq('')]\n",
    "print(f\"Number of empty documents in train set: {empty_docs.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33db2fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'm', 'a', 'r', 'i', 's', 'w', 'a', 'i', 't', 'i', 'n', 'g', 'f', 'o', 'r', 'm', 'e']\n"
     ]
    }
   ],
   "source": [
    "text1 = 'Omar is waiting for me'\n",
    "token1 = bpe.tokenize(text1)\n",
    "print(token1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcb9f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "408fdf3b",
   "metadata": {},
   "source": [
    "WHITE SPACE TOKENIZATION CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d8e0bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordTokenizer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \n",
    "        return text.split()  # Splits on whitespace by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1429606e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-based tokens: ['This', 'is', 'a', 'test', 'sentence.']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "word_tokenizer = WordTokenizer()\n",
    "text = 'This is a test sentence.'\n",
    "tokens = word_tokenizer.tokenize(text)\n",
    "print('Word-based tokens:', tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7474691f",
   "metadata": {},
   "source": [
    "FEATURE EXTRACTION FROM TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44fb9947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing sklearn libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90cd7ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction using count-based features for word-based tokenization\n",
    "word_vectorizer = CountVectorizer()\n",
    "X_train_word = word_vectorizer.fit_transform(train_df['text'])\n",
    "X_dev_word = word_vectorizer.transform(dev_df['text'])\n",
    "X_test_word = word_vectorizer.transform(test_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c71cac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2461)\t1\n",
      "  (0, 757)\t1\n",
      "  (0, 4579)\t1\n",
      "  (0, 1068)\t1\n",
      "  (0, 5403)\t1\n",
      "  (0, 4937)\t1\n",
      "  (0, 1754)\t1\n",
      "  (1, 1005)\t1\n",
      "  (1, 3455)\t1\n",
      "  (1, 747)\t1\n",
      "  (1, 3711)\t1\n",
      "  (1, 3386)\t1\n",
      "  (1, 2994)\t1\n",
      "  (1, 4826)\t1\n",
      "  (1, 2904)\t1\n",
      "  (1, 1180)\t1\n",
      "  (2, 5403)\t1\n",
      "  (2, 4681)\t2\n",
      "  (2, 5513)\t1\n",
      "  (2, 2854)\t1\n",
      "  (2, 5216)\t1\n",
      "  (2, 4862)\t1\n",
      "  (2, 3632)\t1\n",
      "  (2, 3893)\t1\n",
      "  (2, 4687)\t1\n",
      "  :\t:\n",
      "  (2955, 3754)\t1\n",
      "  (2955, 486)\t1\n",
      "  (2955, 2497)\t1\n",
      "  (2956, 232)\t3\n",
      "  (2956, 4691)\t1\n",
      "  (2956, 2243)\t1\n",
      "  (2956, 4764)\t1\n",
      "  (2956, 3389)\t1\n",
      "  (2956, 4851)\t1\n",
      "  (2956, 2675)\t1\n",
      "  (2956, 4204)\t1\n",
      "  (2957, 3711)\t1\n",
      "  (2957, 3229)\t1\n",
      "  (2957, 553)\t1\n",
      "  (2957, 3816)\t1\n",
      "  (2957, 2608)\t1\n",
      "  (2957, 905)\t1\n",
      "  (2958, 3386)\t1\n",
      "  (2958, 3632)\t1\n",
      "  (2958, 3340)\t1\n",
      "  (2958, 1591)\t1\n",
      "  (2958, 3385)\t1\n",
      "  (2958, 5379)\t1\n",
      "  (2958, 3841)\t1\n",
      "  (2958, 3100)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_train_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ea10716",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Feature extraction using count-based features for cleaned BPE tokenization\n",
    "# bpe_vectorizer = CountVectorizer(stop_words=None)\n",
    "bpe_vectorizer = CountVectorizer(stop_words=None, token_pattern=r'\\b\\w+\\b')\n",
    "\n",
    "X_train_bpe = bpe_vectorizer.fit_transform(train_df['bpe_text'])\n",
    "X_dev_bpe = bpe_vectorizer.transform(dev_df['bpe_text'])\n",
    "X_test_bpe = bpe_vectorizer.transform(test_df['bpe_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d7af43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10)\t3\n",
      "  (0, 0)\t2\n",
      "  (0, 14)\t2\n",
      "  (0, 1)\t2\n",
      "  (0, 4)\t2\n",
      "  (0, 18)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 17)\t1\n",
      "  (0, 20)\t2\n",
      "  (0, 19)\t2\n",
      "  (0, 22)\t2\n",
      "  (0, 8)\t1\n",
      "  (1, 0)\t3\n",
      "  (1, 14)\t1\n",
      "  (1, 1)\t2\n",
      "  (1, 4)\t3\n",
      "  (1, 18)\t1\n",
      "  (1, 17)\t2\n",
      "  (1, 20)\t1\n",
      "  (1, 19)\t3\n",
      "  (1, 8)\t4\n",
      "  (1, 13)\t4\n",
      "  (1, 24)\t2\n",
      "  (1, 12)\t2\n",
      "  (1, 15)\t2\n",
      "  :\t:\n",
      "  (2956, 3)\t1\n",
      "  (2957, 10)\t1\n",
      "  (2957, 0)\t4\n",
      "  (2957, 14)\t2\n",
      "  (2957, 1)\t1\n",
      "  (2957, 4)\t1\n",
      "  (2957, 18)\t2\n",
      "  (2957, 20)\t3\n",
      "  (2957, 19)\t1\n",
      "  (2957, 8)\t1\n",
      "  (2957, 13)\t4\n",
      "  (2957, 24)\t1\n",
      "  (2957, 12)\t4\n",
      "  (2957, 6)\t1\n",
      "  (2958, 0)\t1\n",
      "  (2958, 14)\t3\n",
      "  (2958, 4)\t4\n",
      "  (2958, 20)\t2\n",
      "  (2958, 22)\t1\n",
      "  (2958, 8)\t3\n",
      "  (2958, 13)\t5\n",
      "  (2958, 24)\t1\n",
      "  (2958, 12)\t5\n",
      "  (2958, 15)\t2\n",
      "  (2958, 5)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_train_bpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02c9e2d",
   "metadata": {},
   "source": [
    "DEFINING LABLES FOR THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00da1fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "y_train = train_df['label']\n",
    "y_dev = dev_df['label']\n",
    "y_test = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8510961c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       negative\n",
      "1       negative\n",
      "2       negative\n",
      "3       negative\n",
      "4       negative\n",
      "          ...   \n",
      "3476    positive\n",
      "3477    positive\n",
      "3478    positive\n",
      "3479    positive\n",
      "3480    positive\n",
      "Name: label, Length: 2959, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dd156cde",
   "metadata": {},
   "source": [
    "TRAINING NAIVE BAYES CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78883dc7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-based Tokenization - Classification Report on Dev Set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.76      0.74       147\n",
      "    positive       0.80      0.78      0.79       183\n",
      "\n",
      "    accuracy                           0.77       330\n",
      "   macro avg       0.76      0.77      0.76       330\n",
      "weighted avg       0.77      0.77      0.77       330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Naive Bayes classifier for word-based tokenization with Laplace smoothing\n",
    "nb_word = MultinomialNB(alpha=1.0)\n",
    "nb_word.fit(X_train_word, y_train)\n",
    "y_pred_dev_word = nb_word.predict(X_dev_word)\n",
    "print(\"Word-based Tokenization - Classification Report on Dev Set\")\n",
    "print(classification_report(y_dev, y_pred_dev_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c0d4adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\ephra\\Documents\\UCT Jupyter Notebook\\NLP\\afrisent-semeval-2023\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c3bae9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to dev_set_predictions_word.txt\n"
     ]
    }
   ],
   "source": [
    "predictions = nb_word.predict(X_dev_word)\n",
    "\n",
    "output_file = 'dev_set_predictions_word.txt'\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    for text, prediction in zip(dev_df['text'], predictions):\n",
    "        f.write(f\"Sentence: {text}\\nPredicted Sentiment: {prediction}\\n\\n\")\n",
    "\n",
    "print(f\"Predictions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8771e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Tokenization - Classification Report on Dev Set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.46      0.52       147\n",
      "    positive       0.63      0.75      0.69       183\n",
      "\n",
      "    accuracy                           0.62       330\n",
      "   macro avg       0.61      0.60      0.60       330\n",
      "weighted avg       0.61      0.62      0.61       330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Naive Bayes classifier for BPE tokenization with Laplace smoothing\n",
    "nb_bpe = MultinomialNB(alpha=1.0)\n",
    "nb_bpe.fit(X_train_bpe, y_train)\n",
    "y_pred_dev_bpe = nb_bpe.predict(X_dev_bpe)\n",
    "print(\"BPE Tokenization - Classification Report on Dev Set\")\n",
    "print(classification_report(y_dev, y_pred_dev_bpe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5de88e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to dev_set_predictions_bpe.txt\n"
     ]
    }
   ],
   "source": [
    "predictions = nb_bpe.predict(X_dev_bpe)\n",
    "\n",
    "output_file = 'dev_set_predictions_bpe.txt'\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    for text, prediction in zip(dev_df['text'], predictions):\n",
    "        f.write(f\"Sentence: {text}\\nPredicted Sentiment: {prediction}\\n\\n\")\n",
    "\n",
    "print(f\"Predictions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "5a0de9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-based Tokenization - Classification Report on Test Set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.76      0.76       353\n",
      "    positive       0.81      0.81      0.81       450\n",
      "\n",
      "    accuracy                           0.79       803\n",
      "   macro avg       0.79      0.79      0.79       803\n",
      "weighted avg       0.79      0.79      0.79       803\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction and evaluation on the test set for word-based tokenization\n",
    "y_pred_test_word = nb_word.predict(X_test_word)\n",
    "print(\"Word-based Tokenization - Classification Report on Test Set\")\n",
    "print(classification_report(y_test, y_pred_test_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "d144cecb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Tokenization - Classification Report on Test Set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.46      0.51       353\n",
      "    positive       0.63      0.73      0.68       450\n",
      "\n",
      "    accuracy                           0.61       803\n",
      "   macro avg       0.60      0.59      0.59       803\n",
      "weighted avg       0.60      0.61      0.60       803\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction and evaluation on the test set for BPE tokenization\n",
    "y_pred_test_bpe = nb_bpe.predict(X_test_bpe)\n",
    "print(\"BPE Tokenization - Classification Report on Test Set\")\n",
    "print(classification_report(y_test, y_pred_test_bpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad76767f",
   "metadata": {},
   "source": [
    "PREDICTION FUNCTION TO CLASSIFY NEW SENTENCES AS EITHER POSITIVE OR NEGATIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "ec4e8270",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.merges = {}\n",
    "\n",
    "    def load_merges(self, merges_file):\n",
    "        with open(merges_file, 'rb') as f:\n",
    "            self.merges = pickle.load(f)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        pre_tokenized_text = text.split()\n",
    "        splits_text = [[l for l in word] for word in pre_tokenized_text]\n",
    "        for pair, merge in self.merges.items():\n",
    "            for idx, split in enumerate(splits_text):\n",
    "                i = 0\n",
    "                while i < len(split) - 1:\n",
    "                    if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                        split = split[:i] + [merge] + split[i + 2:]\n",
    "                    else:\n",
    "                        i += 1\n",
    "                splits_text[idx] = split\n",
    "        result = sum(splits_text, [])\n",
    "        return result\n",
    "\n",
    "def predict_sentiment_bpe(text, model, vectorizer, tokenizer):\n",
    "    # Load merges\n",
    "    tokenizer.load_merges('bpe_merges.pkl')\n",
    "\n",
    "    # Clean and tokenize the input text using BPE\n",
    "    clean_text = clean(text)\n",
    "    tokenized_text = tokenizer.tokenize(clean_text)\n",
    "    \n",
    "    # Convert tokens to feature vectors using the vectorizer\n",
    "    features = vectorizer.transform([' '.join(tokenized_text)])\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(features)\n",
    "    \n",
    "    return prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "82d2928e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment (BPE): negative\n"
     ]
    }
   ],
   "source": [
    "# Example usage for BPE tokenization\n",
    "tokenizer = BPETokenizer(vocab_size=1000)  # Initialize tokenizer with the desired vocab size\n",
    "text_bpe = \"wo ye gyimi paa\"\n",
    "predicted_sentiment_bpe = predict_sentiment_bpe(text_bpe, nb_bpe, bpe_vectorizer, tokenizer)\n",
    "print(f\"Predicted sentiment (BPE): {predicted_sentiment_bpe}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "5ab42789",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = WordTokenizer()\n",
    "\n",
    "def predict_sentiment_word_based(text, model, vectorizer):\n",
    "\n",
    "    # Clean the input text\n",
    "    clean_text = clean(text)\n",
    "\n",
    "    # Tokenize the cleaned text using WordTokenizer\n",
    "    tokenized_text = word_tokenizer.tokenize(clean_text)\n",
    "\n",
    "    # Convert tokenized text to feature vectors\n",
    "    features = vectorizer.transform([' '.join(tokenized_text)])\n",
    "\n",
    "    # Make prediction\n",
    "    prediction = model.predict(features)\n",
    "    \n",
    "    return prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "94d25a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment (Word-Based): positive\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text_word_based = \"me pe wo wate\"\n",
    "predicted_sentiment_word_based = predict_sentiment_word_based(text_word_based, nb_word, word_vectorizer)\n",
    "print(f\"Predicted sentiment (Word-Based): {predicted_sentiment_word_based}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cccdcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
